{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ephemeral/home/data/df_high_ratio.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  사건!>실\"를 b$#라 #극)]체O.;월f:?\n",
      "1             전W;참c 이nd 유j]\"m객ie((우려` ?<. 개조\n",
      "2             더*} ]i대+ 김현권!v원a}_대8*vdL줄!알았d \n",
      "3             $G ;!p 서울 |곳 나vo객 =적\"bs 귀s,d거)\n",
      "4             nO통령z\\후 p싱턴으로 ^R!與지p부 @re-#{{\\\n",
      "                        ...                 \n",
      "1587    G창 k천K 진입도로 .방도 승격 국P 50V억 지원 4차로 8장\n",
      "1589                     특징주 메가B디(자회사 합병에 급W\n",
      "1590                   ]광공사 페루 여0자J무장T도 R의해Y\n",
      "1592                   프랑스 이란에 핵OW 의무 %0L삼가라\n",
      "1593                  최저임W위 퇴장 |견하는 한국I총 위원B\n",
      "Name: text, Length: 1580, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 영어가 있는 행만 추출하여 확인\n",
    "english_rows = df[df['text'].str.contains(r'[A-Za-z]', regex=True)]\n",
    "\n",
    "print(english_rows['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1            전W;참c 이nd 유j]\"m객ie((우려` ?<. 개조\n",
      "2            더*} ]i대+ 김현권!v원a}_대8*vdL줄!알았d \n",
      "3            $G ;!p 서울 |곳 나vo객 =적\"bs 귀s,d거)\n",
      "4            nO통령z\\후 p싱턴으로 ^R!與지p부 @re-#{{\\\n",
      "5          ^r^[P홀w2019년H'v요?서트vYp 세,#uNs[cJ\n",
      "                       ...                 \n",
      "1579       UEFA 챔스리그 4강 마드리드 더비 성사 레알 vs 아 \n",
      "1582       프로농구 kt 로건 없이도 삼성 완파 벌써 지난해 10승 \n",
      "1584                      KT Be Y 폰 2 단독 출시\n",
      "1586    확진 속출F부산 사하구B모든 학교 이틀간 RV수업3q7\"교 대3\n",
      "1592                  프랑스 이란에 핵OW 의무 %0L삼가라\n",
      "Name: text, Length: 1065, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 두 글자 이상 연속된 영어 알파벳이 포함된 행 필터링\n",
    "english_two_or_more = df[df['text'].str.contains(r'[A-Za-z]{2,}', regex=True)]\n",
    "\n",
    "print(english_two_or_more['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117       R*_jz )침 공]기준'나왔다Pu구자 ]정부담k\n",
      "119               y=sc럼*광합성하h f공장W 개발\n",
      "121    MWC 스키점프nVev `d한PI글라데r1'|x아~나\\\n",
      "122    선두5도X공사*GS칼텍$^K;dhb현Q#i탈7m연승]합\n",
      "123    미국 7R; 멕시g 2hoU년m\\o컵X공\"=W8<청#듯\n",
      "124    밀라노!d지오rd4 광장k`시a j/|I노Q8o옥외광고\n",
      "125    노?u김)d 피QY3손민j 겨4P{c는bd베G트n연$곡\n",
      "126    .달 CES %굴#N바@은^새a|더o폰I중저o폰O rb\n",
      "127    롯데\\cs론트pKB]리din 승o두AON드블럼은e5연}\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 두 글자 이상 연속된 소문자 알파벳이 포함된 행 필터링\n",
    "english_two_or_more = df[df['text'].str.contains(r'[a-z]{2,}', regex=True)]\n",
    "\n",
    "print(english_two_or_more['text'][70:79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MW 스키점프 `한P글라데1'|아~나\n"
     ]
    }
   ],
   "source": [
    "# 대문자 두 글자 이상의 단어나 'vs'를 제외하고 영어를 삭제하는 함수\n",
    "def remove_unwanted_english(text):\n",
    "    # 대문자 두 글자 이상 또는 'vs'는 유지, 나머지 영어 단어는 제거\n",
    "    return re.sub(r'(?![A-Z]{2,}|vs)[A-Za-z]', '', text)\n",
    "\n",
    "# 예제 문장\n",
    "text = \"MWC 스키점프nVev `d한PI글라데r1'|x아~나\"\n",
    "\n",
    "# 함수 적용\n",
    "cleaned_text = remove_unwanted_english(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전참이유객우vs려개조VS이건예시TEXT입니다.TET\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(text):\n",
    "    # 대문자 두 글자 이상의 단어 또는 'vs'를 찾는 정규 표현식\n",
    "    meaningful_pattern = r'[A-Z]{2,}|vs'\n",
    "    \n",
    "    # 결과를 담을 리스트\n",
    "    cleaned_text = []\n",
    "\n",
    "    current_word = ''\n",
    "    for char in text:\n",
    "        if re.match(r'[\\uAC00-\\uD7A3]', char):  # 한글인 경우\n",
    "            if current_word:\n",
    "                # 이전에 쌓인 단어가 있으면 의미 있는지 확인\n",
    "                if re.match(meaningful_pattern, current_word):\n",
    "                    cleaned_text.append(current_word)\n",
    "                current_word = ''  # current_word 초기화\n",
    "            cleaned_text.append(char)  # 한글 추가\n",
    "        elif char.isalpha():  # 영어 알파벳인 경우\n",
    "            current_word += char  # 영어 문자를 current_word에 추가\n",
    "        else:  # 기타 문자는 그대로 유지\n",
    "            if current_word:\n",
    "                if re.match(meaningful_pattern, current_word):\n",
    "                    cleaned_text.append(current_word)\n",
    "                current_word = ''  # current_word 초기화\n",
    "            cleaned_text.append(char)  # 기타 문자는 추가\n",
    "\n",
    "    # 마지막으로 쌓인 current_word가 의미 있는지 확인\n",
    "    if current_word and re.match(meaningful_pattern, current_word):\n",
    "        cleaned_text.append(current_word)\n",
    "\n",
    "    return ''.join(cleaned_text)\n",
    "\n",
    "# 예시 텍스트\n",
    "text = \"전W참c이nd유jm객ie우vs려개조VS이건예시TEXT입니다.TET\"\n",
    "cleaned_text = remove_noise(text)\n",
    "print(cleaned_text)\n",
    "\n",
    "df['cleaned_text']= df['text'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['전', '##참', '##이', '##유', '##객', '##우', '##v', '##s', '##려', '##개', '##조', '##V', '##S', '##이', '##건', '##예', '##시', '##TE', '##X', '##T', '##입니다', '.', 'TE', '##T']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        사건!>실\"를 $#라 #극)]체.;월:?\n",
       "1        전;참 이 유]\"객((우려` ?<. 개조\n",
       "2       더*} ]대+ 김현권!원}_대8*줄!알았 \n",
       "3        $ ;! 서울 |곳 나객 =적\" 귀,거)\n",
       "4         통령\\후 싱턴으로 ^!지부 @-#{{\\\n",
       "                 ...           \n",
       "1590          ]광공사 페루 여0자무장도 의해\n",
       "1591      대한항공 인천∼바르셀로나 노선 첫 취항\n",
       "1592       프랑스 이란에 핵OW 의무 %0삼가라\n",
       "1593        최저임위 퇴장 |견하는 한국총 위원\n",
       "1594    옥션 5만∼10만원 한우 설 선물세트 인기\n",
       "Name: cleaned_text, Length: 1595, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MASK] 토큰으로 노이즈 정상화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT가 채운 문장: 롯데 자이언츠 론트. 리 승 승. 두 번 드블럼은 한 연.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# BERT 모델과 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "model = BertForMaskedLM.from_pretrained('klue/bert-base')\n",
    "\n",
    "# 입력 텍스트 예시\n",
    "text = \"롯데[MASK]론트[MASK]리[MASK] 승[MASK]두[MASK]드블럼은[MASK]연.\"\n",
    "\n",
    "# 텍스트 토큰화 및 입력 ID 생성\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# [MASK] 위치 예측\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "\n",
    "# 각 [MASK] 위치에 대해 가장 높은 확률의 예측 단어 ID를 가져와서 토큰화 해제\n",
    "for mask_index in mask_token_indices:\n",
    "    predicted_token_id = predictions[0, mask_index, :].argmax(axis=-1)\n",
    "    input_ids[0, mask_index] = predicted_token_id\n",
    "\n",
    "# 최종 문장 디코딩\n",
    "final_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"BERT가 채운 문장:\", final_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
