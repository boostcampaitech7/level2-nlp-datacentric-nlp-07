{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm  # 진행 상황을 시각적으로 표시하기 위한 라이브러리\n",
    "\n",
    "# EXAONE 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_csv_path = 'noise.csv'  # 입력 CSV 파일 경로\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "cleaned_texts = []\n",
    "\n",
    "# tqdm을 사용하여 진행 상황을 표시\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Texts\"):\n",
    "    noisy_text = row['text']  # 'text'는 노이즈가 포함된 컬럼 이름입니다.\n",
    "\n",
    "    # 노이즈 제거를 위한 프롬프트 생성\n",
    "    prompt = f\"다음 텍스트에서 불필요한 특수문자나 잡음을 제거하고 정제된 문장으로 수정하세요: '{noisy_text}'\"\n",
    "\n",
    "    # 메시지 형식화\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 입력 텐서 준비\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 모델에서 텍스트 생성 (노이즈 제거)\n",
    "    output = model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 출력 디코딩 및 정제된 텍스트 추출 (프롬프트 부분을 제외하고 실제 정제된 텍스트만 추출)\n",
    "    cleaned_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 정제된 텍스트에서 프롬프트 부분을 제외한 텍스트만 추출\n",
    "    start_idx = cleaned_text.find(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\")\n",
    "    if start_idx != -1:\n",
    "        cleaned_text = cleaned_text[start_idx + len(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\"):].strip()\n",
    "\n",
    "    # 결과 리스트에 추가\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# 정제된 텍스트를 데이터프레임에 추가\n",
    "df['cleaned_text'] = cleaned_texts\n",
    "\n",
    "# 결과 CSV 파일로 저장\n",
    "output_csv_path = 'cleaned_texts.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"노이즈가 제거된 텍스트가 {output_csv_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID                                               text  \\\n",
      "0  ynat-v1_train_00624                                                  실   \n",
      "1  ynat-v1_train_02413                 전 세계가 우려하는 개조: 윤리적 경계에서 벗어난 기술의 미래   \n",
      "2  ynat-v1_train_01203         미국 대통령 선거: 새로운 지도자, 새로운 시대를 향하여 워싱턴으로의 귀환    \n",
      "3  ynat-v1_train_02105  주어진 뉴스 기사 제목은 이해하기 어려운 형태로 보입니다. 이를 바탕으로 유사한 제...   \n",
      "4  ynat-v1_train_02765                   증강된 제목: 서울 관광객 증가, 도시 경제에 긍정적 영향   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       6  \n",
      "2       2  \n",
      "3       0  \n",
      "4       3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_cleaned_text(file_path):\n",
    "    # CSV 파일을 읽어들입니다.\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 'cleaned_text' 열을 처리합니다.\n",
    "    def process_text(text):\n",
    "        # 텍스트에서 모든 따옴표 내부의 내용들을 찾아 리스트로 저장합니다.\n",
    "        matches = re.findall(r\"['\\\"](.*?)['\\\"]\", text)\n",
    "        \n",
    "        if matches:\n",
    "            # 두 번째 따옴표 내부의 내용을 반환 (인덱스 1)\n",
    "            if len(matches) > 1:\n",
    "                return matches[1]\n",
    "            # 하나만 있을 경우 그 하나를 반환\n",
    "            return matches[0]\n",
    "        else:\n",
    "            # 따옴표가 없으면 첫 번째 줄만 반환\n",
    "            return text.split('\\n')[0]\n",
    "    \n",
    "    # 'cleaned_text' 열에 적용\n",
    "    df['augmented_title'] = df['augmented_title'].apply(process_text)\n",
    "    \n",
    "    # 'cleaned_text' 열을 'text' 열로 변경\n",
    "    df['text'] = df['augmented_title']\n",
    "    \n",
    "    # 기존 'text' 열 삭제\n",
    "    df.drop(columns=['augmented_title'], inplace=True)\n",
    "    \n",
    "    # 변경된 DataFrame을 반환\n",
    "    return df\n",
    "\n",
    "# 파일 경로를 지정하여 호출\n",
    "file_path = 'train_aug_aya.csv'  # 실제 파일 경로로 수정 필요\n",
    "processed_df = process_cleaned_text(file_path)\n",
    "\n",
    "# 처리된 데이터프레임 확인 (예시)\n",
    "print(processed_df.head())\n",
    "\n",
    "# 'text' 열에서 '증강된 제목: ' 문자열을 제거\n",
    "processed_df['text'] = processed_df['text'].str.replace('증강된 제목: ', '', regex=False)\n",
    "\n",
    "# 'text' 열에서 길이가 4글자 이하인 행 제거\n",
    "processed_df = processed_df[processed_df['text'].str.len() > 4]\n",
    "\n",
    "# 처리된 결과를 새로운 CSV 파일로 저장 (선택 사항)\n",
    "processed_df.to_csv('exaone_clean_noise.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('./data/noise.csv')\n",
    "\n",
    "model = OllamaLLM(\n",
    "    model='jmpark333/exaone',\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[\\[\\]\"]', '', text).strip()\n",
    "    return cleaned\n",
    "\n",
    "template = \"\"\"\n",
    "    다음은 뉴스 기사 제목입니다:\n",
    "    뉴스 기사 제목: \"{Text}\"\n",
    "    \n",
    "    지시사항:\n",
    "    1. 뉴스 기사 제목을 읽어.\n",
    "    2. 유사한 내용의 제목을 하나 생성해.\n",
    "    \n",
    "    지정 답변 형식: \"[증강된 제목]\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "\n",
    "augmented_texts = []\n",
    "for text in tqdm(data['text'].to_list(), desc='데이터 증강 중...', total=len(data)):\n",
    "    response = chain.invoke({\"Text\": text})\n",
    "    cleaned_response = clean_text(response)\n",
    "    augmented_texts.append(cleaned_response)\n",
    "\n",
    "data['augmented_text'] = augmented_texts\n",
    "data.to_csv('train_aug_exaone.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "증강 중...: 100%|██████████| 1595/1595 [34:44<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증강된 제목이 'train_augmented_titles.csv'에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('./data/noise.csv')\n",
    "\n",
    "# 모델 설정\n",
    "model = OllamaLLM(\n",
    "    model='aya-expanse:8b',  # 모델 변경 필요\n",
    "    temperature=0.7,  # 다양성 조절\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    # 텍스트에서 불필요한 대괄호와 따옴표를 제거하고 공백을 정리\n",
    "    cleaned = re.sub(r'[\\[\\]\"]', '', text).strip()\n",
    "    return cleaned\n",
    "\n",
    "# 증강할 텍스트 형식 정의 (주제 변경 또는 유사 제목 생성)\n",
    "template = \"\"\"\n",
    "    다음은 뉴스 기사 제목입니다.\n",
    "    뉴스 기사 제목: \"{Text}\"\n",
    "    지시사항:\n",
    "    1. 주어진 뉴스 기사 제목을 읽어라.\n",
    "    2. 해당 제목을 바탕으로 유사한 제목을 생성하라.\n",
    "    지정 답변 형식: \"[증강된 제목]\"\n",
    "\"\"\"\n",
    "\n",
    "# 템플릿 생성 및 체인 설정\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "\n",
    "# 데이터 증강\n",
    "augmented_titles = []\n",
    "for text in tqdm(data['text'].to_list(), desc='증강 중...', total=len(data)):\n",
    "    # 모델에 입력하여 응답 받기\n",
    "    response = chain.invoke({\"Text\": text})\n",
    "    \n",
    "    # 응답에서 불필요한 부분 제거\n",
    "    cleaned_response = clean_text(response)\n",
    "    augmented_titles.append(cleaned_response)\n",
    "\n",
    "# 증강된 제목을 원본 데이터에 추가\n",
    "data['augmented_title'] = augmented_titles\n",
    "\n",
    "# 결과 CSV 파일로 저장\n",
    "data.to_csv('train_aug_aya.csv', index=False)\n",
    "\n",
    "print(\"증강된 제목이 'train_augmented_titles.csv'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. 데이터 로드\n",
    "df = pd.read_csv(\"noise_free.csv\")  # noise_free.csv 경로를 정확히 지정하세요\n",
    "texts = df['text'].values  # 텍스트 데이터\n",
    "\n",
    "# 2. KoBERT 모델과 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 3. 텍스트를 KoBERT로 임베딩\n",
    "def embed_text(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # 평균값을 구하여 텍스트 임베딩으로 사용\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# 임베딩 생성\n",
    "embeddings = embed_text(texts)\n",
    "\n",
    "# 4. KMeans 클러스터링\n",
    "num_clusters = 7  # 7개의 클러스터로 분류\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# 5. 클러스터 예측 결과\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# 6. 각 클러스터의 주요 단어 출력 (임베딩을 직접 해석하기 어려운 점을 고려하여 생략)\n",
    "# 대신 각 클러스터별로 제목을 출력하여 확인\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_titles = df[df['cluster'] == i]['text'].tolist()\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(\"\\n\".join(cluster_titles[:5]))  # 각 클러스터의 상위 5개 뉴스 제목 출력\n",
    "    print()\n",
    "\n",
    "# 7. 클러스터링 결과 확인 (선택 사항)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='cluster', data=df)\n",
    "plt.title(\"Cluster Distribution\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 8. 결과 파일로 저장 (선택 사항)\n",
    "df.to_csv(\"clustered_noise_free.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
