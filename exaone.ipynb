{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 00:39:48.487491: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-07 00:39:48.501727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730907588.526506  539460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730907588.533555  539460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 00:39:48.558473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c1da9af5074b938aeb60f8498133b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노이즈가 제거된 텍스트가 cleaned_texts.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# EXAONE 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_csv_path = 'noise.csv'  # 입력 CSV 파일 경로\n",
    "df = pd.read_csv(input_csv_path).head()\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "cleaned_texts = []\n",
    "\n",
    "# 각 텍스트에 대해 노이즈 제거 작업 수행\n",
    "for index, row in df.iterrows():\n",
    "    noisy_text = row['text']  # 'text'는 노이즈가 포함된 컬럼 이름입니다.\n",
    "\n",
    "    # 노이즈 제거를 위한 프롬프트 생성\n",
    "    prompt = f\"다음 텍스트에서 불필요한 특수문자나 잡음을 제거하고 정제된 문장으로 수정하세요: '{noisy_text}'\"\n",
    "\n",
    "    # 메시지 형식화\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 입력 텐서 준비\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 모델에서 텍스트 생성 (노이즈 제거)\n",
    "    output = model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 출력 디코딩 및 정제된 텍스트 추출\n",
    "    cleaned_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 결과 리스트에 추가\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# 정제된 텍스트를 데이터프레임에 추가\n",
    "df['cleaned_text'] = cleaned_texts\n",
    "\n",
    "# 결과 CSV 파일로 저장\n",
    "output_csv_path = 'cleaned_texts.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"노이즈가 제거된 텍스트가 {output_csv_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abde4241ef60468ea42d045762bdc15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노이즈가 제거된 텍스트가 cleaned_texts.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# EXAONE 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_csv_path = 'noise.csv'  # 입력 CSV 파일 경로\n",
    "df = pd.read_csv(input_csv_path).head()\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "cleaned_texts = []\n",
    "\n",
    "# 각 텍스트에 대해 노이즈 제거 작업 수행\n",
    "for index, row in df.iterrows():\n",
    "    noisy_text = row['text']  # 'text'는 노이즈가 포함된 컬럼 이름입니다.\n",
    "\n",
    "    # 노이즈 제거를 위한 프롬프트 생성\n",
    "    prompt = f\"다음 텍스트에서 불필요한 특수문자나 잡음을 제거하고 정제된 문장으로 수정하세요: '{noisy_text}'\"\n",
    "\n",
    "    # 메시지 형식화\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 입력 텐서 준비\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 모델에서 텍스트 생성 (노이즈 제거)\n",
    "    output = model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 출력 디코딩 및 정제된 텍스트 추출 (프롬프트 부분을 제외하고 실제 정제된 텍스트만 추출)\n",
    "    cleaned_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 정제된 텍스트에서 프롬프트 부분을 제외한 텍스트만 추출\n",
    "    start_idx = cleaned_text.find(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\")\n",
    "    if start_idx != -1:\n",
    "        cleaned_text = cleaned_text[start_idx + len(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\"):].strip()\n",
    "\n",
    "    # 결과 리스트에 추가\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# 정제된 텍스트를 데이터프레임에 추가\n",
    "df['cleaned_text'] = cleaned_texts\n",
    "\n",
    "# 결과 CSV 파일로 저장\n",
    "output_csv_path = 'cleaned_texts.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"노이즈가 제거된 텍스트가 {output_csv_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e47c7ce9b2047478e92b557672a3774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Processing Texts:   0%|          | 0/1595 [01:26<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm  # 진행 상황을 시각적으로 표시하기 위한 라이브러리\n",
    "\n",
    "# EXAONE 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_csv_path = 'noise.csv'  # 입력 CSV 파일 경로\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "cleaned_texts = []\n",
    "\n",
    "# tqdm을 사용하여 진행 상황을 표시\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Texts\"):\n",
    "    noisy_text = row['text']  # 'text'는 노이즈가 포함된 컬럼 이름입니다.\n",
    "\n",
    "    # 노이즈 제거를 위한 프롬프트 생성\n",
    "    prompt = f\"다음 텍스트에서 불필요한 특수문자나 잡음을 제거하고 정제된 문장으로 수정하세요: '{noisy_text}'\"\n",
    "\n",
    "    # 메시지 형식화\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 입력 텐서 준비\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 모델에서 텍스트 생성 (노이즈 제거)\n",
    "    output = model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 출력 디코딩 및 정제된 텍스트 추출 (프롬프트 부분을 제외하고 실제 정제된 텍스트만 추출)\n",
    "    cleaned_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # 정제된 텍스트에서 프롬프트 부분을 제외한 텍스트만 추출\n",
    "    start_idx = cleaned_text.find(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\")\n",
    "    if start_idx != -1:\n",
    "        cleaned_text = cleaned_text[start_idx + len(\"다음은 불필요한 특수문자와 잡음을 제거한 정제된 문장입니다:\"):].strip()\n",
    "\n",
    "    # 결과 리스트에 추가\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# 정제된 텍스트를 데이터프레임에 추가\n",
    "df['cleaned_text'] = cleaned_texts\n",
    "\n",
    "# 결과 CSV 파일로 저장\n",
    "output_csv_path = 'cleaned_texts.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"노이즈가 제거된 텍스트가 {output_csv_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드 (train.csv는 ID, text, target 열을 가진 파일)\n",
    "df = pd.read_csv('noise.csv').head(5)\n",
    "\n",
    "# 노이즈 제거 함수 정의\n",
    "def denoise_text(text):\n",
    "    # 시스템 메시지 및 사용자 메시지 구성\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please clean this text: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    # 텍스트를 토큰화하여 모델에 입력\n",
    "    conversation = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "    input_ids = tokenizer(conversation, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    # 모델로부터 출력 생성\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트 디코딩\n",
    "    denoised_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return denoised_text\n",
    "\n",
    "# 진행 상황 표시를 위한 tqdm 사용\n",
    "tqdm.pandas(desc=\"Denoising text\")\n",
    "\n",
    "# 각 텍스트에 대해 노이즈 제거 수행 (진행 현황 표시)\n",
    "df['denoised_text'] = df['text'].progress_apply(denoise_text)\n",
    "\n",
    "# # 노이즈가 제거된 데이터셋을 새로운 CSV 파일로 저장\n",
    "# df.to_csv('denoised_train.csv', index=False)\n",
    "\n",
    "print(\"노이즈 제거 완료! denoised_train.csv 파일에 저장되었습니다.\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드 (상위 10개 데이터만 사용)\n",
    "df = pd.read_csv('noise.csv').head(10)\n",
    "\n",
    "# 노이즈 제거 함수 정의\n",
    "def denoise_text(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please clean this text: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    conversation = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "    input_ids = tokenizer(conversation, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 모델 출력에서 \"user\"와 \"system\" 메시지를 제외하고, 노이즈 제거된 텍스트만 추출\n",
    "    denoised_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 실제 노이즈 제거된 텍스트만 반환 (시작 부분을 제거하고 원하는 부분만 남기기)\n",
    "    start_index = denoised_text.find('```\\n') + 4  # 첫 번째 '```' 이후의 텍스트부터\n",
    "    denoised_text = denoised_text[start_index:].strip()\n",
    "\n",
    "    return denoised_text\n",
    "\n",
    "# 진행 상황 표시를 위한 tqdm 사용\n",
    "tqdm.pandas(desc=\"Denoising text\")\n",
    "\n",
    "# 각 텍스트에 대해 노이즈 제거 수행 (진행 현황 표시)\n",
    "df['denoised_text'] = df['text'].progress_apply(denoise_text)\n",
    "\n",
    "# 노이즈가 제거된 데이터셋을 새로운 CSV 파일로 저장\n",
    "df.to_csv('denoised_train_sample.csv', index=False)\n",
    "\n",
    "print(\"노이즈 제거 완료! denoised_train_sample.csv 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# CSV 파일 로드 (상위 10개 데이터만 사용)\n",
    "df = pd.read_csv('noise.csv').head(10)\n",
    "\n",
    "# 노이즈 제거 함수 정의\n",
    "def denoise_text(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please clean this text: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    conversation = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "    input_ids = tokenizer(conversation, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "\n",
    "    # 모델 출력에서 불필요한 부분을 제외하고, 노이즈 제거된 텍스트만 추출\n",
    "    denoised_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # \"system\"과 \"user\" 역할을 제외하고 실제 노이즈 제거된 텍스트만 추출\n",
    "    # 예시: \"I apologize...\" 같은 부분을 제거\n",
    "    start_index = denoised_text.find(\"Please clean this text:\") + len(\"Please clean this text:\")  # 실제 텍스트 시작 위치 찾기\n",
    "    denoised_text = denoised_text[start_index:].strip()\n",
    "\n",
    "    return denoised_text\n",
    "\n",
    "# 진행 상황 표시를 위한 tqdm 사용\n",
    "tqdm.pandas(desc=\"Denoising text\")\n",
    "\n",
    "# 각 텍스트에 대해 노이즈 제거 수행 (진행 현황 표시)\n",
    "df['denoised_text'] = df['text'].progress_apply(denoise_text)\n",
    "\n",
    "# 노이즈가 제거된 데이터셋을 새로운 CSV 파일로 저장\n",
    "df.to_csv('denoised_train_sample.csv', index=False)\n",
    "\n",
    "print(\"노이즈 제거 완료! denoised_train_sample.csv 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['denoised_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. 데이터 로드\n",
    "df = pd.read_csv(\"noise_free.csv\")  # noise_free.csv 경로를 정확히 지정하세요\n",
    "texts = df['text'].values  # 텍스트 데이터\n",
    "\n",
    "# 2. KoBERT 모델과 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 3. 텍스트를 KoBERT로 임베딩\n",
    "def embed_text(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # 평균값을 구하여 텍스트 임베딩으로 사용\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# 임베딩 생성\n",
    "embeddings = embed_text(texts)\n",
    "\n",
    "# 4. KMeans 클러스터링\n",
    "num_clusters = 7  # 7개의 클러스터로 분류\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# 5. 클러스터 예측 결과\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# 6. 각 클러스터의 주요 단어 출력 (임베딩을 직접 해석하기 어려운 점을 고려하여 생략)\n",
    "# 대신 각 클러스터별로 제목을 출력하여 확인\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_titles = df[df['cluster'] == i]['text'].tolist()\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(\"\\n\".join(cluster_titles[:5]))  # 각 클러스터의 상위 5개 뉴스 제목 출력\n",
    "    print()\n",
    "\n",
    "# 7. 클러스터링 결과 확인 (선택 사항)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='cluster', data=df)\n",
    "plt.title(\"Cluster Distribution\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 8. 결과 파일로 저장 (선택 사항)\n",
    "df.to_csv(\"clustered_noise_free.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
