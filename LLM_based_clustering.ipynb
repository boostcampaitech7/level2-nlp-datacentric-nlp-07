{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic idea\n",
    "- 기존 ML 기반 클러스터링은 잘 작동하지 않았음\n",
    "- 텍스트가 주제 분류될 때 복잡한 과정을 거치기 때문 ex) KT wiz와 삼성 라이온즈의 맞대결 승자는? &rarr; 스포츠; 하지만 KT와 삼성이라는 단어 때문에 경제로 분류될 가능성 있음.\n",
    "- 복잡한 로직의 처리를 LLM으로 하면 어떨까? &rarr; LLM 사용 시 출력 컨트롤이 관건\n",
    "    1. 출력이 영어로 되는 경우 - 프롬프트에서 한국어 명시\n",
    "    2. 출력이 균일하게 되지 않는 경우 - seed와 temperature 관리\n",
    "    3. 출력에 노이즈가 생기는 경우 - 노이즈 양에 따라 허용 or 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from filter import SpecialCharFilter\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_csv('data/train.csv')\n",
    "\n",
    "special_char_filter = SpecialCharFilter()\n",
    "\n",
    "noise_df = special_char_filter.filter_noise(df)\n",
    "clean_df = df[~df.index.isin(noise_df.index)]\n",
    "print(len(noise_df), len(clean_df))\n",
    "noise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant that categories news article titles to propre sections. \n",
    "            only say in a short korean word.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{input}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:27b\",\n",
    "    seed=42,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = \"data/clustered.csv\"\n",
    "\n",
    "if not os.path.exists(cluster_path):\n",
    "    for idx, row in clean_df.iterrows():\n",
    "        input_text = row['text']\n",
    "        ai_msg = chain.invoke({\"input\": input_text})\n",
    "        predicted_label = ai_msg.content.strip()  # 결과 문자열에서 공백 제거\n",
    "        clean_df.loc[idx, 'predict_label'] = re.sub(r'[^가-힣a-zA-Z\\s]', '', predicted_label)\n",
    "        print(f\"Index: {idx}, Input: {input_text}, Predicted Label: {predicted_label}\")\n",
    "else:\n",
    "    clean_df = pd.read_csv(cluster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 실행 결과를 보면, 다양한 label로 분류되는 걸 알 수 있다. 따라서 label 텍스트 자체를 임베딩해 클러스터링을 시도한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 1: BERT embedding + K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('data/clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = clean_df['predict_label'].value_counts()\n",
    "label_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klue/bert-base 모델을 활용해 토큰화 및 임베딩 후 k-means 알고리즘으로 클러스터링을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"klue/bert-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32, padding=\"max_length\")\n",
    "#     outputs = model(**inputs)\n",
    "#     # [CLS] 토큰의 출력만 사용하여 임베딩 생성\n",
    "#     embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy()\n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = clean_df['predict_label'].astype(str).apply(get_embedding).tolist()\n",
    "# embeddings = torch.tensor(embeddings)  # 리스트를 텐서로 변환\n",
    "\n",
    "# # KMeans 클러스터링 (7개의 클러스터로 설정)\n",
    "# kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "# clean_df['label_cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# # 클러스터링 결과 확인\n",
    "# print(clean_df[['predict_label', 'label_cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_counts = clean_df['label_cluster'].value_counts()\n",
    "# print(\"\\nCluster distribution:\")\n",
    "# print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_tab = pd.crosstab(clean_df['label_cluster'], clean_df['target'])\n",
    "# cross_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클러스터링 실험 결과 사전에 분석한 라벨 분포와 다르다. 사전 분석 시 라벨이 골고루 분포되어 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 2: LLM 라벨 예측 결과 분류 - 대화 정보 기억\n",
    "\n",
    "LLM의 출력 결과를 7가지로 추리기 위해서 어떤 기능을 사용해야 할까?\n",
    "LLM agent에서 버퍼 메모리 기능 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(titles, session_id):\n",
    "    prompt = (\n",
    "        \"다음은 뉴스 기사 제목들의 목록입니다:\\n\\n\"\n",
    "        + \"\\n\".join(titles)\n",
    "        + \"\\n\\n이 제목들을 7개의 대표적인 카테고리로 분류하여, 각 카테고리의 이름만 한 단어로 출력하세요.\"\n",
    "    )\n",
    "    response = chain.invoke(\n",
    "        prompt,\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    return response.content.strip()\n",
    "\n",
    "def classify_title(title, categories, session_id):\n",
    "    prompt = (\n",
    "        f\"다음은 뉴스 기사 제목입니다:\\n\\n\"\n",
    "        f\"제목: {title}\\n\\n\"\n",
    "        f\"반드시 아래의 카테고리 중 하나로 분류하고, 해당 카테고리의 이름만 한 단어로 출력하세요:\\n{', '.join(categories)}\"\n",
    "    )\n",
    "    response = chain.invoke(\n",
    "        prompt,\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 기사 제목 리스트\n",
    "titles = clean_df['text'].to_list()\n",
    "\n",
    "# 세션 ID 설정\n",
    "session_id = \"1\"\n",
    "\n",
    "if not os.path.exists('data/clustered_7_labels.csv'):\n",
    "    # 1. 카테고리 추출\n",
    "    categories = extract_categories(titles, session_id)\n",
    "    print(\"추출된 카테고리:\\n\", categories)\n",
    "\n",
    "    # 2. 각 제목에 대한 분류\n",
    "    classified_categories = []\n",
    "    for title in tqdm(titles):\n",
    "        category = classify_title(title, categories, session_id)\n",
    "        classified_categories.append(category)\n",
    "        print(f\"제목: {title}\\n분류된 카테고리: {category}\\n\")\n",
    "        \n",
    "    clean_df['label_cluster'] = classified_categories\n",
    "    clean_df.to_csv('data/clustered_7_labels.csv')\n",
    "else:\n",
    "    clean_df = pd.read_csv('data/clustered_7_labels.csv')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백그라운드 실행 명령어\n",
    "```bash\n",
    "nohup ollama serve &\n",
    "nohup jupyter nbconvert --to notebook --execute LLM_based_clustering.ipynb --output LLM_based_clustering_output.ipynb &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = clean_df['label_cluster'].value_counts()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_labels = label_count.head(7).index.to_list()\n",
    "extra = clean_df[~clean_df['label_cluster'].isin(most_labels)]\n",
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra['label_cluster'] = extra['label_cluster'].apply(lambda x: classify_title(x, most_labels, session_id))\n",
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = extra['label_cluster'].value_counts()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.update(extra[['label_cluster']])\n",
    "\n",
    "columns_to_drop = ['Unnamed: 0.4', 'Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1' ,\n",
    "                   'special_char_count', 'special_char_ratio', 'predict_label']\n",
    "clean_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = clean_df['label_cluster'].value_counts()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('data/clustered_7_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 분석\n",
    "## base idea\n",
    "- ASCII 오염된 데이터(이하 노이즈 데이터)에서 라벨 클러스터링을 통해 'target' 칼럼에 있는 정수 인코딩 정보를 알 수 있었다.\n",
    "- 실험 2에서 분류된 카테고리와 노이즈 데이터 라벨 클러스터링 카테고리가 문맥 상 거의 일치했다.\n",
    "    실험 2 결과: 경제, 스포츠, 정치, 국제, 문화, 사회, 과학\n",
    "    노이즈 데이터 클러스터링 결과: *승범 데이터 받을 것*\n",
    "- 위 정보를 바탕으로 간단한 EDA와 시각화를 실시했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = pd.read_csv('data/clustered_7_labels.csv')\n",
    "\n",
    "# ASCII 데이터 예측을 통해 정수 인코딩이 의미하는 실제 카테고리 내용 파악 완료\n",
    "clustered_df['target'] = clustered_df['target'].apply(lambda x: {0: '생활문화', 1: '스포츠', 2: '정치', 3: '사회', 4: 'IT과학', 5: '경제', 6: '세계'}.get(x, x))\n",
    "\n",
    "# 위 실험 결과로 매핑 완료\n",
    "clustered_df['label_cluster'] = clustered_df['label_cluster'].apply(\n",
    "    lambda x: {\n",
    "        \"경제\": \"경제\", \n",
    "        \"스포츠\": \"스포츠\", \n",
    "        \"정치\": \"정치\", \n",
    "        \"국제\": \"세계\", \n",
    "        \"문화\": \"생활문화\", \n",
    "        \"사회\": \"사회\", \n",
    "        \"과학\": \"IT과학\",\n",
    "    }.get(x, x)\n",
    ")\n",
    "\n",
    "clustered_df = clustered_df[['ID', 'text', 'target', 'label_cluster']]\n",
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 라벨링 정보와 실험 2 결과 클러스터링 차이\n",
    "# 데이터 설명에서 정상 데이터는 200개라고 설명함\n",
    "len(clustered_df[clustered_df['target'] == clustered_df['label_cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font='AppleGothic', rc={'axes.unicode_minus': False})\n",
    "\n",
    "unique_clusters = clustered_df['label_cluster'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(len(unique_clusters), 1, figsize=(10, len(unique_clusters) * 4))\n",
    "fig.suptitle(\"Target Distribution by Label Cluster\", fontsize=16)\n",
    "\n",
    "for i, cluster in enumerate(unique_clusters):\n",
    "    cluster_data = clustered_df[clustered_df['label_cluster'] == cluster]\n",
    "    sns.countplot(data=cluster_data, x='target', ax=axes[i])\n",
    "    axes[i].set_title(f\"Label Cluster: {cluster}\")\n",
    "    axes[i].set_xlabel(\"Target\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-centric-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
